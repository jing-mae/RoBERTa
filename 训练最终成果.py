# -*- coding: utf-8 -*-
"""è®­ç»ƒæœ€ç»ˆæˆæžœ

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VR6ICaCa-Krx3o99N2m2YTgOQfk9tS9f
"""

from IPython import get_ipython
from IPython.display import display
# %%
!pip install torch transformers pandas scikit-learn matplotlib --quiet

import pandas as pd
import numpy as np
import torch
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report, ConfusionMatrixDisplay
from sklearn.utils.class_weight import compute_class_weight
import matplotlib.pyplot as plt
import os
import random
import torch.nn as nn # Import nn module

# %%
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")
# %%
torch.manual_seed(42)
# %%
from google.colab import drive
drive.mount('/content/drive')

# è®¾ç½®è·¯å¾„
DATA_PATH = "/content/drive/MyDrive/cleaned_final_with_tags_v5.csv"

# è¯»å– CSV æ–‡ä»¶
df = pd.read_csv(DATA_PATH)
print(f"Loaded {len(df)} samples.")
print(df.head())

# %%
merge_map = {
    "neo_nazi": "white_supremacy",
    "nationalist_extremism": "white_supremacy",
    "racism": "white_supremacy",
    "xenophobia": "white_supremacy",
    "anti_semitic": "white_supremacy"
}

df['all_hate_tags'] = df['all_hate_tags'].apply(lambda tags: list(set([merge_map.get(tag, tag) for tag in eval(tags)])))

merge_map = {
    "neo_nazi": "white_supremacy",
    "nationalist_extremism": "white_supremacy",
    "racism": "white_supremacy",
    "xenophobia": "white_supremacy",
    "anti_semitic": "white_supremacy"
}

# Assuming 'all_hate_tags' is already a list after the previous step
df['all_hate_tags'] = df['all_hate_tags'].apply(lambda tags: list(set([merge_map.get(tag, tag) for tag in tags])))

# æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
from collections import Counter
# Corrected list comprehension
all_tags = [tag for tag_list in df['all_hate_tags'] for tag in tag_list]
tag_counts = Counter(all_tags)
print("æ ‡ç­¾åˆ†å¸ƒï¼š")
print(tag_counts)

labels_flat = [tag for sublist in df['all_hate_tags'] for tag in sublist]
unique_labels = np.array(list(set(labels_flat)))
class_weights = compute_class_weight(class_weight='balanced', classes=unique_labels, y=labels_flat)

weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)

# è½¬æ¢ä¸ºå­—å…¸
label_weights = dict(zip(unique_labels, class_weights))
print("æ ‡ç­¾æƒé‡ï¼š")
print(label_weights)

def augment_text(text):
    if isinstance(text, str):
        words = text.split()
        if len(words) > 5:
            num_to_remove = random.randint(1, 3)
            for _ in range(num_to_remove):
                words.pop(random.randint(0, len(words) - 1))
        return " ".join(words)
    return text

# æ•°æ®å¢žå¼º
df_augmented = df.copy()
df_augmented['lyrcis'] = df_augmented['lyrcis'].apply(augment_text)

df_combined = pd.concat([df, df_augmented], ignore_index=True)

print(f"Data after augmentation: {len(df_combined)} samples.")

mlb = MultiLabelBinarizer()
multi_hot_labels = mlb.fit_transform(df_combined['all_hate_tags'])

# é‡æ–°ç”Ÿæˆ DataFrame
final_df = pd.concat([df_combined[['lyrcis']], pd.DataFrame(multi_hot_labels, columns=mlb.classes_)], axis=1)

# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
# Filter out non-string entries from the 'lyrcis' column before splitting
valid_indices = final_df['lyrcis'].apply(lambda x: isinstance(x, str))
filtered_df = final_df[valid_indices]

train_texts, val_texts, train_labels, val_labels = train_test_split(
    filtered_df['lyrcis'].tolist(),
    filtered_df.drop(columns=['lyrcis']).values.tolist(),
    test_size=0.2,
    random_state=42
)

print(f"Train samples: {len(train_labels)}, Validation samples: {len(val_labels)}")

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")

class HateLyricsMultiLabelDataset(torch.utils.data.Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx],
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors="pt"
        )
        encoding = {key: val.squeeze(0) for key, val in encoding.items()}
        encoding['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)
        return encoding

train_dataset = HateLyricsMultiLabelDataset(train_texts, train_labels, tokenizer)
val_dataset = HateLyricsMultiLabelDataset(val_texts, val_labels, tokenizer)

print(f"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}")

from transformers import RobertaConfig, RobertaForSequenceClassification

num_labels = len(label_weights)
config = RobertaConfig.from_pretrained(
    "roberta-base",
    num_labels=num_labels,
    hidden_dropout_prob=0.4,
    attention_probs_dropout_prob=0.4
)

class CustomRobertaForMultiLabel(RobertaForSequenceClassification):
    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):
        # åªä¼ éœ€è¦çš„å‚æ•°ç»™çˆ¶ç±»
        outputs = super().forward(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        logits = outputs.logits

        # å¤šæ ‡ç­¾ loss è®¡ç®—
        loss = None
        if labels is not None:
            loss_fct = nn.BCEWithLogitsLoss(pos_weight=weights_tensor)
            loss = loss_fct(logits, labels)

        return {"loss": loss, "logits": logits}

model = CustomRobertaForMultiLabel.from_pretrained("roberta-base", config=config)
model.to(device)

print(f"Model initialized with {num_labels} labels and enhanced dropout.")

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=20,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    warmup_steps=200,
    weight_decay=0.01,
    learning_rate=1e-5,
    # evaluation_strategy="epoch", # Changed to eval_strategy for compatibility with older transformers versions
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_dir="./logs",
    logging_steps=10,
    load_best_model_at_end=True
)

print("Training arguments configured.")

# %%
from transformers import TrainingArguments, DataCollatorWithPadding, Trainer
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score # Ensure these are imported

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = (logits > 0).astype(int)
    precision = precision_score(labels, preds, average='macro', zero_division=0)
    recall = recall_score(labels, preds, average='macro', zero_division=0)
    f1 = f1_score(labels, preds, average='macro', zero_division=0)
    accuracy = accuracy_score(labels, preds)
    return {
        "accuracy": accuracy,
        "f1": f1,
        "precision": precision,
        "recall": recall
    }

collator = DataCollatorWithPadding(tokenizer)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
)

print("Trainer initialized with custom loss function. Starting training...")

trainer.train()
print("Training complete.")

metrics = trainer.evaluate()
print(metrics)

# ä¿å­˜æ¨¡åž‹å’Œ tokenizer
model_path = "/content/hate_roberta_model"
model.save_pretrained(model_path)
tokenizer.save_pretrained(model_path)

# æ‰“åŒ…ä¸º zip
!zip -r hate_roberta_model.zip /content/hate_roberta_model

# ä¸‹è½½åˆ°æœ¬åœ°
from google.colab import files
files.download("hate_roberta_model.zip")

print("âœ… æ¨¡åž‹å·²ä¿å­˜ä¸ºæœ¬åœ°æ–‡ä»¶ hate_roberta_model.zip")

from google.colab import drive
drive.mount('/content/drive')

!cp -r /content/hate_roberta_model /content/drive/MyDrive/

import os

model_path = "/content/hate_roberta_model"
print("ðŸ“‚ æ–‡ä»¶åˆ—è¡¨ï¼š")
print(os.listdir(model_path))

from google.colab import files

uploaded = files.upload()

df_eval = pd.read_csv("eva_final.csv")
df_eval = df_eval.dropna(subset=['lyrics', 'label'])
print(f"Loaded {len(df_eval)} evaluation samples.")

import pandas as pd

# âœ… åŠ è½½æµ‹è¯•æ•°æ®
df_eval = pd.read_csv("eva_final.csv")

# âœ… åˆå¹¶æ ‡ç­¾åˆ—ä¸ºåˆ—è¡¨
def collect_tags(row):
    return list(filter(pd.notna, [row['hate_label'], row['hate_label2'], row['hate_label3']]))

df_eval['all_tags'] = df_eval.apply(collect_tags, axis=1)

from sklearn.preprocessing import MultiLabelBinarizer

# æ‰€æœ‰å¯èƒ½çš„æ ‡ç­¾ï¼ˆå¿…é¡»å’Œè®­ç»ƒæ—¶ä¸€è‡´ï¼‰
label_classes = ['white_supremacy', 'religious', 'extreme_violence', 'mythological_justification']

mlb = MultiLabelBinarizer(classes=label_classes)
true_binary = mlb.fit_transform(df_eval["all_tags"])

from google.colab import drive
drive.mount('/content/drive')

model_path = "/content/drive/MyDrive/hate_roberta_model"

from transformers import RobertaTokenizer, RobertaForSequenceClassification

tokenizer = RobertaTokenizer.from_pretrained(model_path, local_files_only=True)
model = RobertaForSequenceClassification.from_pretrained(model_path, local_files_only=True)
model.to("cuda" if torch.cuda.is_available() else "cpu")

# é¢„å¤„ç†æ­Œè¯
lyrics = df_eval["lyrics"].tolist()

encodings = tokenizer(
    lyrics,
    truncation=True,
    padding=True,
    max_length=512,
    return_tensors="pt"
)

encodings = {k: v.to(model.device) for k, v in encodings.items()}

# é¢„æµ‹
with torch.no_grad():
    outputs = model(**encodings)
    logits = outputs.logits
    probs = torch.sigmoid(logits).cpu().numpy()

# å°†æ¦‚çŽ‡è½¬æ¢ä¸º 0/1 æ ‡ç­¾
threshold = 0.5
predictions = (probs >= threshold).astype(int)

from sklearn.metrics import classification_report

print("ðŸ“Š Evaluation Report:")
print(classification_report(true_binary, predictions, target_names=mlb.classes_))