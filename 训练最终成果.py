# -*- coding: utf-8 -*-
"""训练最终成果

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VR6ICaCa-Krx3o99N2m2YTgOQfk9tS9f
"""

from IPython import get_ipython
from IPython.display import display
# %%
!pip install torch transformers pandas scikit-learn matplotlib --quiet

import pandas as pd
import numpy as np
import torch
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report, ConfusionMatrixDisplay
from sklearn.utils.class_weight import compute_class_weight
import matplotlib.pyplot as plt
import os
import random
import torch.nn as nn # Import nn module

# %%
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")
# %%
torch.manual_seed(42)
# %%
from google.colab import drive
drive.mount('/content/drive')

# 设置路径
DATA_PATH = "/content/drive/MyDrive/cleaned_final_with_tags_v5.csv"

# 读取 CSV 文件
df = pd.read_csv(DATA_PATH)
print(f"Loaded {len(df)} samples.")
print(df.head())

# %%
merge_map = {
    "neo_nazi": "white_supremacy",
    "nationalist_extremism": "white_supremacy",
    "racism": "white_supremacy",
    "xenophobia": "white_supremacy",
    "anti_semitic": "white_supremacy"
}

df['all_hate_tags'] = df['all_hate_tags'].apply(lambda tags: list(set([merge_map.get(tag, tag) for tag in eval(tags)])))

merge_map = {
    "neo_nazi": "white_supremacy",
    "nationalist_extremism": "white_supremacy",
    "racism": "white_supremacy",
    "xenophobia": "white_supremacy",
    "anti_semitic": "white_supremacy"
}

# Assuming 'all_hate_tags' is already a list after the previous step
df['all_hate_tags'] = df['all_hate_tags'].apply(lambda tags: list(set([merge_map.get(tag, tag) for tag in tags])))

# 检查标签分布
from collections import Counter
# Corrected list comprehension
all_tags = [tag for tag_list in df['all_hate_tags'] for tag in tag_list]
tag_counts = Counter(all_tags)
print("标签分布：")
print(tag_counts)

labels_flat = [tag for sublist in df['all_hate_tags'] for tag in sublist]
unique_labels = np.array(list(set(labels_flat)))
class_weights = compute_class_weight(class_weight='balanced', classes=unique_labels, y=labels_flat)

weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)

# 转换为字典
label_weights = dict(zip(unique_labels, class_weights))
print("标签权重：")
print(label_weights)

def augment_text(text):
    if isinstance(text, str):
        words = text.split()
        if len(words) > 5:
            num_to_remove = random.randint(1, 3)
            for _ in range(num_to_remove):
                words.pop(random.randint(0, len(words) - 1))
        return " ".join(words)
    return text

# 数据增强
df_augmented = df.copy()
df_augmented['lyrcis'] = df_augmented['lyrcis'].apply(augment_text)

df_combined = pd.concat([df, df_augmented], ignore_index=True)

print(f"Data after augmentation: {len(df_combined)} samples.")

mlb = MultiLabelBinarizer()
multi_hot_labels = mlb.fit_transform(df_combined['all_hate_tags'])

# 重新生成 DataFrame
final_df = pd.concat([df_combined[['lyrcis']], pd.DataFrame(multi_hot_labels, columns=mlb.classes_)], axis=1)

# 划分训练集和验证集
# Filter out non-string entries from the 'lyrcis' column before splitting
valid_indices = final_df['lyrcis'].apply(lambda x: isinstance(x, str))
filtered_df = final_df[valid_indices]

train_texts, val_texts, train_labels, val_labels = train_test_split(
    filtered_df['lyrcis'].tolist(),
    filtered_df.drop(columns=['lyrcis']).values.tolist(),
    test_size=0.2,
    random_state=42
)

print(f"Train samples: {len(train_labels)}, Validation samples: {len(val_labels)}")

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")

class HateLyricsMultiLabelDataset(torch.utils.data.Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx],
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors="pt"
        )
        encoding = {key: val.squeeze(0) for key, val in encoding.items()}
        encoding['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)
        return encoding

train_dataset = HateLyricsMultiLabelDataset(train_texts, train_labels, tokenizer)
val_dataset = HateLyricsMultiLabelDataset(val_texts, val_labels, tokenizer)

print(f"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}")

from transformers import RobertaConfig, RobertaForSequenceClassification

num_labels = len(label_weights)
config = RobertaConfig.from_pretrained(
    "roberta-base",
    num_labels=num_labels,
    hidden_dropout_prob=0.4,
    attention_probs_dropout_prob=0.4
)

class CustomRobertaForMultiLabel(RobertaForSequenceClassification):
    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):
        # 只传需要的参数给父类
        outputs = super().forward(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        logits = outputs.logits

        # 多标签 loss 计算
        loss = None
        if labels is not None:
            loss_fct = nn.BCEWithLogitsLoss(pos_weight=weights_tensor)
            loss = loss_fct(logits, labels)

        return {"loss": loss, "logits": logits}

model = CustomRobertaForMultiLabel.from_pretrained("roberta-base", config=config)
model.to(device)

print(f"Model initialized with {num_labels} labels and enhanced dropout.")

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=20,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    warmup_steps=200,
    weight_decay=0.01,
    learning_rate=1e-5,
    # evaluation_strategy="epoch", # Changed to eval_strategy for compatibility with older transformers versions
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_dir="./logs",
    logging_steps=10,
    load_best_model_at_end=True
)

print("Training arguments configured.")

# %%
from transformers import TrainingArguments, DataCollatorWithPadding, Trainer
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score # Ensure these are imported

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = (logits > 0).astype(int)
    precision = precision_score(labels, preds, average='macro', zero_division=0)
    recall = recall_score(labels, preds, average='macro', zero_division=0)
    f1 = f1_score(labels, preds, average='macro', zero_division=0)
    accuracy = accuracy_score(labels, preds)
    return {
        "accuracy": accuracy,
        "f1": f1,
        "precision": precision,
        "recall": recall
    }

collator = DataCollatorWithPadding(tokenizer)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
)

print("Trainer initialized with custom loss function. Starting training...")

trainer.train()
print("Training complete.")

metrics = trainer.evaluate()
print(metrics)

# 保存模型和 tokenizer
model_path = "/content/hate_roberta_model"
model.save_pretrained(model_path)
tokenizer.save_pretrained(model_path)

# 打包为 zip
!zip -r hate_roberta_model.zip /content/hate_roberta_model

# 下载到本地
from google.colab import files
files.download("hate_roberta_model.zip")

print("✅ 模型已保存为本地文件 hate_roberta_model.zip")

from google.colab import drive
drive.mount('/content/drive')

!cp -r /content/hate_roberta_model /content/drive/MyDrive/

import os

model_path = "/content/hate_roberta_model"
print("📂 文件列表：")
print(os.listdir(model_path))

from google.colab import files

uploaded = files.upload()

df_eval = pd.read_csv("eva_final.csv")
df_eval = df_eval.dropna(subset=['lyrics', 'label'])
print(f"Loaded {len(df_eval)} evaluation samples.")

import pandas as pd

# ✅ 加载测试数据
df_eval = pd.read_csv("eva_final.csv")

# ✅ 合并标签列为列表
def collect_tags(row):
    return list(filter(pd.notna, [row['hate_label'], row['hate_label2'], row['hate_label3']]))

df_eval['all_tags'] = df_eval.apply(collect_tags, axis=1)

from sklearn.preprocessing import MultiLabelBinarizer

# 所有可能的标签（必须和训练时一致）
label_classes = ['white_supremacy', 'religious', 'extreme_violence', 'mythological_justification']

mlb = MultiLabelBinarizer(classes=label_classes)
true_binary = mlb.fit_transform(df_eval["all_tags"])

from google.colab import drive
drive.mount('/content/drive')

model_path = "/content/drive/MyDrive/hate_roberta_model"

from transformers import RobertaTokenizer, RobertaForSequenceClassification

tokenizer = RobertaTokenizer.from_pretrained(model_path, local_files_only=True)
model = RobertaForSequenceClassification.from_pretrained(model_path, local_files_only=True)
model.to("cuda" if torch.cuda.is_available() else "cpu")

# 预处理歌词
lyrics = df_eval["lyrics"].tolist()

encodings = tokenizer(
    lyrics,
    truncation=True,
    padding=True,
    max_length=512,
    return_tensors="pt"
)

encodings = {k: v.to(model.device) for k, v in encodings.items()}

# 预测
with torch.no_grad():
    outputs = model(**encodings)
    logits = outputs.logits
    probs = torch.sigmoid(logits).cpu().numpy()

# 将概率转换为 0/1 标签
threshold = 0.5
predictions = (probs >= threshold).astype(int)

from sklearn.metrics import classification_report

print("📊 Evaluation Report:")
print(classification_report(true_binary, predictions, target_names=mlb.classes_))